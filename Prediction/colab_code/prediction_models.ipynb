{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prediction_models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "HamZH4V4kZIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Bidirectional LSTM Model\n",
        "\n",
        "#importing all libraries and modules required \n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Bidirectional, TimeDistributed\n",
        "\n",
        "#function to convert 3-format input data to 5-format\n",
        "def convert_data(data,max_len):\n",
        "\t\"\"\"Pad the data to be stroke-5 bigger format as described in paper.\"\"\"\n",
        "\tstart_stroke_token = [0, 0, 1, 0, 0]\n",
        "\tresult = np.zeros((data.shape[0], max_len + 1, 5), dtype=float)\n",
        "\tfor i in range(data.shape[0]):\n",
        "\t\tl = len(data[i])\n",
        "\t\tassert l <= max_len\n",
        "\t\tresult[i, 0:l, 0:2] = data[i][:, 0:2]\n",
        "\t\tresult[i, 0:l, 3] = data[i][:, 2]\n",
        "\t\tresult[i, 0:l, 2] = 1 - result[i, 0:l, 3]\n",
        "\t\tresult[i, l:, 4] = 1\n",
        "\t\t# put in the first token, as described in sketch-rnn methodology\n",
        "\t\tresult[i, 1:, :] = result[i, :-1, :]\n",
        "\t\tresult[i, 0, :] = 0\n",
        "\t\tresult[i, 0, 2] = start_stroke_token[2]  # setting S_0 from paper.\n",
        "\t\tresult[i, 0, 3] = start_stroke_token[3]\n",
        "\t\tresult[i, 0, 4] = start_stroke_token[4]\n",
        "\treturn result\n",
        "\n",
        "#load the datset\n",
        "folder = \"/content/drive/My Drive/AML/\"\n",
        "with np.load(folder+\"airplane.npz\",encoding='latin1') as data:\n",
        "\ttrain_data = data['train']\n",
        "\tvalid_data = data['valid']\t\n",
        "\ttest_data = data['test']\n",
        "\n",
        "#training parameters\n",
        "batch_size = 250\n",
        "epochs = 30\n",
        "\n",
        "# hyperparmeters of the model\n",
        "num_encoder_tokens = 5\n",
        "num_decoder_tokens = 5\n",
        "intermediate_dim = 128\n",
        "\n",
        "#find max_seq_length in the data\n",
        "max_seq_len = 0\n",
        "all_data = np.concatenate((train_data,valid_data,test_data))\n",
        "for instance in all_data:\n",
        "\tshape = np.shape(instance)\n",
        "\tif shape[0] > max_seq_len:\n",
        "\t\tmax_seq_len  = shape[0]\n",
        "\n",
        "print(\"Max Sequence Length:\",max_seq_len)\n",
        "\n",
        "#convert 3-format data to 5-format\n",
        "train_data = convert_data(train_data, max_seq_len)\n",
        "test_data = convert_data(test_data, max_seq_len)\n",
        "valid_data = convert_data(valid_data, max_seq_len)\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "input_layer = Input(shape=(None, num_encoder_tokens), name = \"input_layer\")\n",
        "lstm_layer = Bidirectional(LSTM(intermediate_dim, return_sequences=True, name = \"LSTM_layer\"))\n",
        "lstm_outputs = lstm_layer(input_layer)\n",
        "decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation='relu', name = \"output_layer\"))\n",
        "decoder_outputs = decoder_dense(lstm_outputs)\n",
        "model = Model([input_layer], decoder_outputs)\n",
        "\n",
        "#compile\n",
        "model.compile(optimizer='rmsprop',loss = 'mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "#train\n",
        "model.fit(train_data[:,:max_seq_len,:], train_data[:,1:max_seq_len+1,:],\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(valid_data[:,:max_seq_len,:], valid_data[:,1:max_seq_len+1,:]))\n",
        "\n",
        "#test\n",
        "print(model.evaluate(test_data[:,:max_seq_len,:],test_data[:,1:max_seq_len+1,:],batch_size=batch_size))\n",
        "\n",
        "  \n",
        "#save the model\n",
        "model.save(folder+\"model/bidirectional_lstm.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z3IFlvieO_CF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#LSTM Autoencoder Model (Sequence to Sequence)\n",
        "\n",
        "#importing all libraries and modules required \n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import Add, Activation\n",
        "from keras.losses import mse, kullback_leibler_divergence\n",
        "\n",
        "#function to convert 3-format input data to 5-format\n",
        "def convert_data(data,max_len):\n",
        "\t\"\"\"Pad the data to be stroke-5 bigger format as described in paper.\"\"\"\n",
        "\tstart_stroke_token = [0, 0, 1, 0, 0]\n",
        "\tresult = np.zeros((data.shape[0], max_len + 1, 5), dtype=float)\n",
        "\tfor i in range(data.shape[0]):\n",
        "\t\tl = len(data[i])\n",
        "\t\tassert l <= max_len\n",
        "\t\tresult[i, 0:l, 0:2] = data[i][:, 0:2]\n",
        "\t\tresult[i, 0:l, 3] = data[i][:, 2]\n",
        "\t\tresult[i, 0:l, 2] = 1 - result[i, 0:l, 3]\n",
        "\t\tresult[i, l:, 4] = 1\n",
        "\t\t# put in the first token, as described in sketch-rnn methodology\n",
        "\t\tresult[i, 1:, :] = result[i, :-1, :]\n",
        "\t\tresult[i, 0, :] = 0\n",
        "\t\tresult[i, 0, 2] = start_stroke_token[2]  # setting S_0 from paper.\n",
        "\t\tresult[i, 0, 3] = start_stroke_token[3]\n",
        "\t\tresult[i, 0, 4] = start_stroke_token[4]\n",
        "\treturn result\n",
        "\n",
        "#load the dataset\n",
        "folder = \"/content/drive/My Drive/AML/\"\n",
        "with np.load(folder+\"cat.npz\",encoding='latin1') as data:\n",
        "\ttrain_data = data['train']\n",
        "\tvalid_data = data['valid']\t\n",
        "\ttest_data = data['test']\n",
        "\n",
        "#training parameters\n",
        "batch_size = 250\n",
        "epochs = 100\n",
        "\n",
        "# configure\n",
        "num_encoder_tokens = 5\n",
        "num_decoder_tokens = 5\n",
        "intermediate_dim = 256\n",
        "\n",
        "#find max_seq_length in the data\n",
        "max_seq_len = 0\n",
        "all_data = np.concatenate((train_data,valid_data,test_data))\n",
        "for instance in all_data:\n",
        "\tshape = np.shape(instance)\n",
        "\tif shape[0] > max_seq_len:\n",
        "\t\tmax_seq_len  = shape[0]\n",
        "\n",
        "print(\"Max Sequence Length:\",max_seq_len)\n",
        "\n",
        "#convert 3-format data to 5-format\n",
        "train_data = convert_data(train_data, max_seq_len)\n",
        "test_data = convert_data(test_data, max_seq_len)\n",
        "valid_data = convert_data(valid_data, max_seq_len)\n",
        "\n",
        "# Define the model.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens), name = \"encoder_input\")\n",
        "encoder = LSTM(intermediate_dim, return_sequences=True, return_state=True, name = \"encoder_lstm\")\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens), name = \"decoder_input\")\n",
        "decoder_lstm = LSTM(intermediate_dim, return_sequences=True, return_state=True, name = \"decoder_lstm\")\n",
        "decoder_output,_,_ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='relu', name = \"output_layer\")\n",
        "decoder_outputs = decoder_dense(decoder_output)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#compile\n",
        "model.compile(optimizer='rmsprop',loss = 'mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train\n",
        "model.fit([train_data[:,:max_seq_len,:],train_data[:,:max_seq_len,:]], train_data[:,1:max_seq_len+1,:],\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=([valid_data[:,:max_seq_len,:], valid_data[:,:max_seq_len,:]], valid_data[:,1:max_seq_len+1,:]))\n",
        "\n",
        "#test\n",
        "print(model.evaluate([test_data[:,:max_seq_len,:], test_data[:,:max_seq_len,:]],test_data[:,1:max_seq_len+1,:],batch_size=batch_size))\n",
        "\n",
        "#save the model  \n",
        "model.save(folder+\"model/autoencoder_lstm.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TFvilgRB7guk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Variational Autoencoder (Sequence to Sequence)\n",
        "\n",
        "#importing all libraries and modules required \n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Lambda\n",
        "from keras.losses import mse, kullback_leibler_divergence\n",
        "\n",
        "#function to convert 3-format input data to 5-format\n",
        "def convert_data(data,max_len):\n",
        "\t\"\"\"Pad the data to be stroke-5 bigger format as described in paper.\"\"\"\n",
        "\tstart_stroke_token = [0, 0, 1, 0, 0]\n",
        "\tresult = np.zeros((data.shape[0], max_len + 1, 5), dtype=float)\n",
        "\tfor i in range(data.shape[0]):\n",
        "\t\tl = len(data[i])\n",
        "\t\tassert l <= max_len\n",
        "\t\tresult[i, 0:l, 0:2] = data[i][:, 0:2]\n",
        "\t\tresult[i, 0:l, 3] = data[i][:, 2]\n",
        "\t\tresult[i, 0:l, 2] = 1 - result[i, 0:l, 3]\n",
        "\t\tresult[i, l:, 4] = 1\n",
        "\t\t# put in the first token, as described in sketch-rnn methodology\n",
        "\t\tresult[i, 1:, :] = result[i, :-1, :]\n",
        "\t\tresult[i, 0, :] = 0\n",
        "\t\tresult[i, 0, 2] = start_stroke_token[2]  # setting S_0 from paper.\n",
        "\t\tresult[i, 0, 3] = start_stroke_token[3]\n",
        "\t\tresult[i, 0, 4] = start_stroke_token[4]\n",
        "\treturn result\n",
        "\n",
        "#load the dataset\n",
        "folder = \"/content/drive/My Drive/AML/\"\n",
        "with np.load(folder+\"cat.npz\",encoding='latin1') as data:\n",
        "\ttrain_data = data['train']\n",
        "\tvalid_data = data['valid']\t\n",
        "\ttest_data = data['test']\n",
        "\n",
        "batch_size = 250\n",
        "epochs = 100\n",
        "\n",
        "# configure\n",
        "num_encoder_tokens = 5\n",
        "num_decoder_tokens = 5\n",
        "intermediate_dim = 256\n",
        "latent_dim = 5\n",
        "\n",
        "#find max_seq_length in the data\n",
        "max_seq_len = 0\n",
        "all_data = np.concatenate((train_data,valid_data,test_data))\n",
        "for instance in all_data:\n",
        "\tshape = np.shape(instance)\n",
        "\tif shape[0] > max_seq_len:\n",
        "\t\tmax_seq_len  = shape[0]\n",
        "\n",
        "print(\"Max Sequence Length:\",max_seq_len)\n",
        "\n",
        "#conert 3-format data to 5-format\n",
        "train_data = convert_data(train_data, max_seq_len)\n",
        "test_data = convert_data(test_data, max_seq_len)\n",
        "valid_data = convert_data(valid_data, max_seq_len)\n",
        "\n",
        "# Define the model.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_input\")\n",
        "encoder = LSTM(intermediate_dim, return_sequences=True, return_state=True, name=\"encoder_lstm\")\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "z_mean = Dense(latent_dim)(encoder_outputs)\n",
        "z_log_var = Dense(latent_dim, activation='relu')(encoder_outputs)\n",
        "\n",
        "def  sampling(args):\n",
        "\tz_mean, z_log_var = args\n",
        "\tbatch = K.shape(z_mean)[0]\n",
        "\ttimesteps = K.shape(z_mean)[1]\n",
        "\tdim = K.int_shape(z_mean)[2]\n",
        "\tepsilon = K.random_normal(shape = (batch, timesteps, dim), mean = 0.)\n",
        "\treturn z_mean + K.exp(z_log_var/2) * epsilon\n",
        "\n",
        "z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "decoder_inputs = Input(shape = (None,num_decoder_tokens), name = \"decoder_input\")\n",
        "decoder_lstm = LSTM(intermediate_dim, return_sequences=True, return_state=True, name = \"decoder_lstm\")\n",
        "decoder_output,_,_ = decoder_lstm(z, initial_state = encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='relu')\n",
        "decoder_outputs = decoder_dense(decoder_output)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "#loss function\n",
        "def get_loss(encoder_inputs, decoder_outputs):\n",
        "\tmse_loss = mse(encoder_inputs, decoder_outputs)\n",
        "\tkl_loss = kullback_leibler_divergence(encoder_inputs, decoder_outputs)\n",
        "\treturn mse_loss + kl_loss\n",
        "\n",
        "#compile\n",
        "model.compile(optimizer='adam',loss = get_loss, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#train\n",
        "model.fit([train_data[:,:max_seq_len,:], train_data[:,:max_seq_len,:]], train_data[:,1:max_seq_len+1,:],\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=([valid_data[:,:max_seq_len,:], valid_data[:,:max_seq_len,:]], valid_data[:,1:max_seq_len+1,:]))\n",
        "\n",
        "#test\n",
        "print(model.evaluate([test_data[:,:max_seq_len,:], test_data[:,:max_seq_len,:]],test_data[:,1:max_seq_len+1,:],batch_size=batch_size))\n",
        "\n",
        "\n",
        "#save the whole model  \n",
        "model.save(folder+\"model/VAE.h5\")\n",
        "\n",
        "#define encoder model\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "#define decoder model\n",
        "decoder_state_input_h = Input(shape=(intermediate_dim,))\n",
        "decoder_state_input_c = Input(shape=(intermediate_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "sampling_inputs = Input(shape=(latent_dim,))\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    sampling_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([sampling_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "#save encoder and decoder models\n",
        "encoder_model.save(folder+\"model/VAEencoder.h5\")\n",
        "decoder_model.save(folder+\"model/VAEdecoder.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}